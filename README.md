# FALMA: Fact-Aware Language Model Adaptation

**FALMA** is a reinforcement learning-based memory management system for dialogue systems that learns to selectively preserve the most important information when memory is constrained. FALMA significantly outperforms GPT's native summarization and other baseline methods, achieving **95% accuracy** with only 50% of the original dialogue content.

## üéØ Key Results

### Performance Comparison (20 dialogues, validation set)

| Method | 25% Memory | 50% Memory | 75% Memory |
|--------|------------|------------|------------|
| **FALMA (Pruning)** | 30% | **95%** ‚úÖ | **95%** ‚úÖ |
| **FALMA (Rate-Distortion)** | 35% | 85% | **95%** ‚úÖ |
| **Custom Summary** | 80% | 85% | 85% |
| **GPT Summary** | 5% | 5% | 45% |
| **Token Buffer** | 5% | 5% | 25% |

**Key Findings:**
- **FALMA pruning achieves 95% accuracy at 50% and 75% memory**, outperforming all baseline methods
- **10 percentage points better** than custom summary at 50% memory (95% vs 85%)
- **50 percentage points better** than GPT's native summarization at 50% memory (95% vs 45%)
- FALMA learns that **Relevance and Emphasis** are the primary importance factors, while **Recency is essentially irrelevant** (0-4% weight)

## üìã Overview

FALMA uses reinforcement learning (PPO) to learn optimal weights for four impact factors that determine the importance of dialogue facts:

1. **Surprisal (S)**: How unexpected or novel the information is
2. **Recency (R)**: Temporal position in the dialogue
3. **Relevance (Q)**: Topic relevance and clarity
4. **Emphasis (E)**: Strength, assertiveness, or emotional force

The system offers two variants:
- **Pruning**: Selectively removes less important facts (binary decisions)
- **Rate-Distortion**: Condenses facts based on importance (continuous decisions)

## üöÄ Quick Start

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/toorajhelmi/alma.git
cd FALMA
```

2. **Set up virtual environment:**
```bash
python3 -m venv venv_py311
source venv_py311/bin/activate  # On Windows: venv_py311\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements/rl.txt
```

4. **Set environment variables:**
```bash
export OPENAI_API_KEY="your-api-key-here"
export TOGETHER_API_KEY="your-together-api-key-here"  # Optional, for Llama models
```

### Basic Usage

**Run FALMA on a dialogue:**
```bash
python main.py
```

**Train FALMA weights for a specific memory ratio:**
```bash
python training/train_ofalma_rl.py \
    --dataset data/dataset_100.json \
    --output models/ofalma_pruning_50 \
    --timesteps 10000 \
    --buffer-size 180 \
    --model gpt-4o
```

**Run comparison experiment:**
```bash
python experiments/varying_mem_experiment.py
```

## üìÅ Code Structure

```
FALMA/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ ofalma.py                    # Core FALMA implementation
‚îÇ   ‚îú‚îÄ‚îÄ token_buffer_memory.py       # Token buffer baseline
‚îÇ   ‚îú‚îÄ‚îÄ token_buffer_memory_with_summary.py  # GPT summary baseline
‚îÇ   ‚îî‚îÄ‚îÄ token_buffer_memory_custom_summary.py  # Custom summary baseline
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ train_ofalma_rl.py          # PPO training script
‚îÇ   ‚îî‚îÄ‚îÄ ofalma_env.py               # RL environment
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îî‚îÄ‚îÄ varying_mem_experiment.py   # Comparison experiment
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ data_generator.py           # Dataset generation
‚îÇ   ‚îî‚îÄ‚îÄ extract_current_weights.py  # Extract learned weights
‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îî‚îÄ‚îÄ ...                         # LLM provider abstractions
‚îî‚îÄ‚îÄ main.py                         # Main entry point
```

### Key Components

#### `core/ofalma.py`
- `apply_ofalma()`: Pruning-based memory management
- `apply_ofalma_rate_distortion()`: Rate-distortion-based memory management
- `compute_impact_scores()`: Computes importance scores using learned weights
- `llm_impact_factors()`: LLM-based computation of S, Q, E factors

#### `training/train_ofalma_rl.py`
- PPO training loop with adaptive learning rate
- Automatic train/validation split (80/20)
- Periodic evaluation and checkpointing
- Logs weights and validation accuracy every 500 steps

#### `training/ofalma_env.py`
- Gymnasium-compatible RL environment
- Computes rewards based on question-answering accuracy
- Handles encoding computation and caching

## üî¨ How It Works

### 1. Impact Factor Computation

For each fact in a dialogue, FALMA computes four impact factors:
- **S, Q, E**: Computed via LLM analysis of the dialogue
- **R**: Computed from position: `(index + 1) / total_facts`

### 2. Importance Score Calculation

The importance score is computed using learned weights:
```
importance = sigmoid(Œ∏_S √ó S + Œ∏_R √ó R + Œ∏_Q √ó Q + Œ∏_E √ó E)
```

### 3. Memory Management

**Pruning variant:**
- Sorts facts by importance score
- Keeps top facts until token budget is reached
- Removes remaining facts

**Rate-Distortion variant:**
- Computes condensation ratio for each fact based on importance
- Higher importance ‚Üí less condensation (more tokens preserved)
- Condenses facts using a smaller LLM (e.g., Llama 3.1 8B)

### 4. Reinforcement Learning

FALMA uses PPO to learn optimal weights:
- **State**: Dialogue representation + current memory ratio
- **Action**: 4D continuous vector (Œ∏_S, Œ∏_R, Œ∏_Q, Œ∏_E)
- **Reward**: +1 for correct answer, -1 for incorrect
- **Training**: 10,000 timesteps with adaptive learning rate

## üìä Learned Weight Analysis

### Pruning Method Weights

| Memory Ratio | Relevance (Q) | Emphasis (E) | Surprisal (S) | Recency (R) |
|--------------|---------------|--------------|---------------|-------------|
| 25% | 43.8% | **52.8%** | 1.7% | 1.6% |
| 50% | **60.8%** | 37.8% | 1.4% | **0.0%** |
| 75% | **51.4%** | 36.8% | 11.5% | 0.3% |

**Key Insights:**
- Relevance and Emphasis dominate (80-98% of total weight)
- Recency is essentially irrelevant (0-1.6% weight)
- At low memory, Emphasis is prioritized; at higher memory, Relevance dominates

### Rate-Distortion Method Weights

| Memory Ratio | Relevance (Q) | Emphasis (E) | Surprisal (S) | Recency (R) |
|--------------|---------------|--------------|---------------|-------------|
| 25% | 46.1% | 46.0% | 4.1% | 3.8% |
| 50% | **53.5%** | 41.1% | 2.7% | 2.7% |
| 75% | 39.3% | 38.3% | **18.2%** | 4.2% |

**Key Insights:**
- More balanced distribution than pruning
- Surprisal receives more weight (especially at 75% memory)
- Condensation allows preservation of unexpected information

## üéì Training Your Own Weights

### Step 1: Prepare Dataset

Your dataset should be a JSON file with the following format:
```json
[
  {
    "id": 1,
    "dialogue": [
      "Fact 1",
      "Fact 2",
      ...
      "Question?"
    ],
    "answer": 42
  },
  ...
]
```

### Step 2: Train for Specific Memory Ratio

```bash
python training/train_ofalma_rl.py \
    --dataset data/dataset_100.json \
    --output models/ofalma_pruning_50 \
    --timesteps 10000 \
    --buffer-size 180 \          # 50% of average dialogue length
    --model gpt-4o \
    --n-envs 4 \
    --learning-rate 1e-3 \
    --lr-decay-factor 0.7 \
    --lr-patience 2
```

### Step 3: Extract Learned Weights

```bash
python scripts/extract_current_weights.py
```

The weights will be saved in the model directory and can be used in your experiments.

## üìà Evaluation

### Run Comparison Experiment

```bash
python experiments/varying_mem_experiment.py
```

This will:
- Load dialogues from the validation set (indices 80-99)
- Test all methods at 25%, 50%, and 75% memory ratios
- Save results to `experiments_logs/varying_mem_experiment_*.json`

### Results Format

Results are saved as JSON with:
- Accuracy for each method at each memory ratio
- Average token usage
- Per-dialogue results with expected vs predicted answers

## üîç Why FALMA Outperforms GPT Summarization

1. **Learned Importance Criteria**: FALMA learns which factors matter most (Relevance and Emphasis) rather than relying on generic summarization heuristics

2. **Task-Specific Optimization**: Weights are optimized for question-answering accuracy, not general text summarization

3. **Adaptive Strategy**: Different weights for different memory ratios allow optimal performance across constraints

4. **Selective Preservation**: Pruning variant makes binary decisions about what to keep, avoiding information loss from summarization

5. **No Information Distortion**: Unlike summarization, which may introduce errors or lose details, pruning preserves original facts exactly

## üìö Documentation

- **[RL Training Guide](docs/README_RL.md)**: Detailed guide for training FALMA weights
- **[Installation Guide](docs/INSTALL_RL.md)**: Step-by-step installation instructions

## üß™ Experimental Results Summary

### Performance Highlights

- **95% accuracy** at 50% memory (19/20 correct on validation set)
- **95% accuracy** at 75% memory (19/20 correct)
- **10 percentage points** better than custom summary at 50% memory
- **50 percentage points** better than GPT's native summarization at 50% memory
- **Near-zero Recency weight** (0-4%) - temporal position is irrelevant for importance

### Token Efficiency

- FALMA pruning uses **92-95% of available token budget** efficiently
- Achieves optimal accuracy with **50% of original content**
- No accuracy improvement beyond 50% memory (indicating optimal information selection)

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìÑ License

[Add your license here]

## üôè Acknowledgments

This work demonstrates the effectiveness of reinforcement learning for optimizing memory management in dialogue systems, showing that learned importance-based selection significantly outperforms traditional summarization approaches.

